## Pipeline ‚Äî PySpark & PostgreSQL

Este projeto consiste em criar um Pipeline de Engenharia de Dados que simula um ambiente banc√°rio, 
utilizando **dados fict√≠cios**, **PySpark para processamento** e **PostgreSQL como camada anal√≠tica**, seguindo boas pr√°ticas de arquitetura em camadas (Bronze, Silver e Gold).

---

###  Objetivo do Projeto

Demonstrar, de forma pr√°tica, como funciona um pipeline de dados banc√°rio do ponto de vista de **engenharia de dados**:

- ingest√£o de dados transacionais fict√≠cios
- aplica√ß√£o de regras de neg√≥cio
- valida√ß√£o e padroniza√ß√£o de dados
- modelagem anal√≠tica
- carga em banco de dados para consumo

O projeto tem foco em **arquitetura, organiza√ß√£o e l√≥gica de neg√≥cio**, e n√£o em dados reais.

---

### Tecnologias Utilizadas

- Python 3.10.19
- PySpark 4.1.1
- PostgreSQL

---

### Tabelas Existentes no Projeto

<p align="center">
  <img src="docs/pipeline_modelagem.png" alt="Modelagem" width="800"/>
</p>

---

### Arquitetura do Pipeline

<p align="center">
  <img src="docs/pipeline_bancario.png" alt="Arquitetura do Pipeline Banc√°rio" width="800"/>
</p>

---

### Camadas de Dados

#### Bronze
- Dados gerados em arquivos CSV
- Estrutura imut√°vel (append-only)
- Representa a ingest√£o inicial de dados banc√°rios

Esta camada simula um **Data Lake banc√°rio**.

#### Silver
- Transforma√ß√µes realizadas com **PySpark**
- Aplica√ß√£o de regras de neg√≥cio
- Padroniza√ß√£o de colunas
- Remo√ß√£o de duplicidades
- Valida√ß√µes de qualidade

Camada de dados confi√°veis, pronta para modelagem anal√≠tica.

#### Gold ‚Äî Analytics
- Dados modelados para consumo
- Carregamento no **PostgreSQL**

Esta camada representa um **Data Warehouse anal√≠tico banc√°rio**.

---

### Conceitos Demonstrados

- Arquitetura em camadas (Bronze, Silver, Gold)
- Processamento distribu√≠do com PySpark
- Regras de neg√≥cio aplicadas em dados financeiros
- Separa√ß√£o entre ingest√£o, tratamento e consumo

---

### Como Executar o Projeto (Resumo)

1. Gerar os dados fict√≠cios em CSV
2. Executar o processamento com PySpark (Bronze ‚Üí Silver)
3. Criar as tabelas anal√≠ticas no PostgreSQL
4. Carregar os dados finais na camada Gold
5. Consultar os dados via SQL

---

## Observa√ß√µes Importantes

- Todos os dados utilizados s√£o **fict√≠cios**
- O PostgreSQL √© utilizado como **simula√ß√£o de um Data Warehouse**
- A arquitetura foi pensada para refletir pr√°ticas reais do setor banc√°rio
- O foco do projeto √© **engenharia de dados**, n√£o visualiza√ß√£o.

---

## üë§ Autor 
Andr√© Silva

Projeto desenvolvido para fins de estudos.